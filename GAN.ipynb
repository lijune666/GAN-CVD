{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzKOhWzSBZHP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.init as init\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7jCDt6BUBb7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_knn = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/df_knn.csv')"
      ],
      "metadata": {
        "id": "CyhcMmMYBdgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split feature and label\n",
        "X = df_knn.drop('CVD0010', axis=1)\n",
        "y = df_knn['CVD0010']"
      ],
      "metadata": {
        "id": "mamexaLNBevI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize X_train, X_test\n",
        "scaler = StandardScaler()\n",
        "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
      ],
      "metadata": {
        "id": "EbCSzvcgBgEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
      ],
      "metadata": {
        "id": "TEsgEhZ4BhDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter\n",
        "n_epochs = 4000\n",
        "batch_size = 128\n",
        "latent_dim = X_train.shape[1]"
      ],
      "metadata": {
        "id": "F9KFYCW_Bh9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GAN model\n",
        "# generator_block\n",
        "def get_generator_block(input_dim, output_dim):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(input_dim, output_dim),\n",
        "        nn.BatchNorm1d(output_dim),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "    )\n",
        "# generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=latent_dim, data_dim=latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            get_generator_block(100, 128),\n",
        "            get_generator_block(128, 256),\n",
        "            get_generator_block(256, 512),\n",
        "            get_generator_block(512, 1024),\n",
        "            get_generator_block(1024, 2048),\n",
        "            nn.Linear(2048, data_dim),\n",
        "        )\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.kaiming_uniform_(m.weight.data, nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    init.zeros_(m.bias.data)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        return self.gen(noise)\n",
        "\n",
        "# discriminator_block\n",
        "def get_discriminator_block(input_dim, output_dim):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(input_dim, output_dim),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout(0.5)\n",
        "    )\n",
        "# 2.discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, data_dim=latent_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            get_discriminator_block(data_dim, 1024),\n",
        "            get_discriminator_block(1024, 512),\n",
        "            get_discriminator_block(512, 256),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, data):\n",
        "        return self.disc(data)"
      ],
      "metadata": {
        "id": "PGOOxqX5XXL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Optimizers\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.00001, betas=(0.5, 0.999))\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.00001, betas=(0.5, 0.999))\n",
        "\n",
        "\n",
        "indices = (y_train == 1)\n",
        "X_real = X_train[indices]\n",
        "y_real = y_train[indices]\n",
        "my_dataset = TensorDataset(torch.Tensor(X_real.values), torch.Tensor(y_real.values))\n",
        "# my_dataset = TensorDataset(torch.Tensor(X_real), torch.Tensor(y_real))\n",
        "# dataloader\n",
        "dataloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "s_N2kct1BkO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original gan Train\n",
        "gen_losses = []\n",
        "disc_losses = []\n",
        "for epoch in range(n_epochs):\n",
        "    for i, real_data in enumerate(dataloader):\n",
        "        real_data = real_data[0].float()\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        real_labels = torch.ones(real_data.size(0), 1, requires_grad=False)\n",
        "        fake_labels = torch.zeros(real_data.size(0), 1, requires_grad=False)\n",
        "\n",
        "        # ---------------------\n",
        "        #  First Train Generator Twice\n",
        "        # ---------------------\n",
        "\n",
        "        for _ in range(2):\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            # Sample noise as generator input\n",
        "            z = torch.randn(real_data.size(0), 100)\n",
        "\n",
        "            # Generate a batch of datas\n",
        "            gen_datas = generator(z)\n",
        "\n",
        "            # Loss measures generator's ability to fool the discriminator\n",
        "            g_loss = criterion(discriminator(gen_datas), real_labels)\n",
        "\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "        # -----------------\n",
        "        #  Then Train Discriminator\n",
        "        # -----------------\n",
        "\n",
        "        d_optimizer.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = torch.randn(real_data.size(0), 100)\n",
        "\n",
        "        # Generate a batch of datas\n",
        "        gen_datas = generator(z)\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = criterion(discriminator(real_data), real_labels)\n",
        "\n",
        "        fake_loss = criterion(discriminator(gen_datas.detach()), fake_labels)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        gen_losses.append(g_loss.item())\n",
        "        disc_losses.append(d_loss.item())\n",
        "\n",
        "        print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "        )\n"
      ],
      "metadata": {
        "id": "a-L1t_9zBl1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists to NumPy arrays\n",
        "gen_losses = np.array(gen_losses)\n",
        "disc_losses = np.array(disc_losses)\n",
        "\n",
        "# Plot loss curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(range(len(gen_losses)), gen_losses, label='Generator loss')\n",
        "plt.plot(range(len(disc_losses)), disc_losses, label='Discriminator loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tRZq5npaBm5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_one = X[y == 1]\n",
        "real_zero = X[y == 0]\n",
        "\n",
        "# count the frequency of each value in y_train.\n",
        "value_counts = y_train.value_counts()\n",
        "# calculate how much fewer the number of minority classes is than that of majority classes.\n",
        "diff = value_counts[0] - value_counts[1]\n",
        "\n",
        "# generate data\n",
        "z = torch.randn(diff, 100)\n",
        "generated_data = generator(z)\n",
        "generated_data_df = pd.DataFrame(generated_data.detach().numpy(), columns=X_train.columns)"
      ],
      "metadata": {
        "id": "KsEG2ZmQBoLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t-SNE\n",
        "# original data (label==1)\n",
        "tsne_real = real_one # num == 76\n",
        "\n",
        "# smote data (label==1)\n",
        "tsne_real_gan = generated_data_df # num == 392\n",
        "\n",
        "# TSNE\n",
        "combined_data = pd.concat([tsne_real_gan, tsne_real], axis=0)\n",
        "labels = np.array([0] * len(tsne_real_gan) + [1] * len(tsne_real))\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_data = tsne.fit_transform(combined_data)\n",
        "\n",
        "# show\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(tsne_data[labels == 0, 0], tsne_data[labels == 0, 1], c='b', label='GAN data')\n",
        "plt.scatter(tsne_data[labels == 1, 0], tsne_data[labels == 1, 1], c='r', label='Real data')\n",
        "plt.legend()\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('TSNE comparing real data with GAN generated data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "axlLTuqVBpHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# merge two datas\n",
        "merged_data = np.concatenate((real_one, generated_data_df), axis=0)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)  # dim==2\n",
        "\n",
        "pca.fit(merged_data)\n",
        "reduced_data = pca.transform(merged_data)\n",
        "\n",
        "# show\n",
        "plt.scatter(reduced_data[real_one.shape[0]:, 0], reduced_data[real_one.shape[0]:, 1], label='GAN data')\n",
        "plt.scatter(reduced_data[:real_one.shape[0], 0], reduced_data[:real_one.shape[0], 1], label='Real data')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dJQ8n1qJBqDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the frequency of each value in y_train.\n",
        "value_counts = y_train.value_counts()\n",
        "# calculate how much fewer the number of minority classes is than that of majority classes.\n",
        "diff = value_counts[0] - value_counts[1]\n",
        "\n",
        "# generate data\n",
        "z = torch.randn(diff, 100)\n",
        "generated_data = generator(z)\n",
        "generated_data_df = pd.DataFrame(generated_data.detach().numpy(), columns=X_train.columns)\n",
        "\n",
        "# concat generated_data_df to X_train\n",
        "X_train_gan = pd.concat([X_train, generated_data_df], axis=0)\n",
        "\n",
        "# Supplement y_train with 1.0\n",
        "ones = pd.Series([1.0] * diff)\n",
        "y_train_gan = y_train.append(ones, ignore_index=True)"
      ],
      "metadata": {
        "id": "qQ4gnVmCBswj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "# shuffle\n",
        "X_train_gan, y_train_gan = shuffle(X_train_gan, y_train_gan, random_state=42)"
      ],
      "metadata": {
        "id": "B3knp-P_Bt8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "import torch\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score, accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def train_and_evaluate_svm(X_train, y_train, X_test, y_test):\n",
        "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "\n",
        "    # SVM\n",
        "    svm_classifier = SVC(probability=True)\n",
        "\n",
        "    # train\n",
        "    svm_classifier.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "    # test\n",
        "    y_scores = svm_classifier.predict_proba(X_test_tensor.numpy())[:, 1]\n",
        "    y_pred = svm_classifier.predict(X_test_tensor.numpy())\n",
        "\n",
        "    # FPR and TPR for ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test_tensor.numpy(), y_scores)\n",
        "\n",
        "    # AUC\n",
        "    auc = roc_auc_score(y_test_tensor.numpy(), y_scores)\n",
        "\n",
        "    return auc\n"
      ],
      "metadata": {
        "id": "gxYwZpCTBxWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original data\n",
        "roc_results = []\n",
        "\n",
        "# Repeat the training and evaluation process 30 times.\n",
        "num_repeats = 30\n",
        "for _ in range(num_repeats):\n",
        "    auc = train_and_evaluate_svm(X_train, y_train, X_test, y_test)\n",
        "    roc_results.append(auc)  # Store the AUC value in the tuple\n",
        "\n",
        "# Calculate the average value of evaluation indicators\n",
        "mean_auc = np.mean([result for result in roc_results])  # Retrieve the AUC value from the tuple\n",
        "\n",
        "print(\"Mean AUC:\", mean_auc)"
      ],
      "metadata": {
        "id": "bPsLlQQKKrTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original data\n",
        "roc_results_gan = []\n",
        "\n",
        "# Repeat the training and evaluation process 30 times.\n",
        "num_repeats = 30\n",
        "for _ in range(num_repeats):\n",
        "    auc = train_and_evaluate_svm(X_train_gan, y_train_gan, X_test, y_test)\n",
        "    roc_results_gan.append(auc)  # Store the AUC value in the tuple\n",
        "\n",
        "# Calculate the average value of evaluation indicators\n",
        "mean_auc_gan = np.mean([result for result in roc_results_gan])  # Retrieve the AUC value from the tuple\n",
        "\n",
        "print(\"Mean AUC:\", mean_auc_gan)"
      ],
      "metadata": {
        "id": "wuMRL58cBz4y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}