{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9alkjWrSoQj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.init as init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_0Q7QHCaHZg"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImPCAzeISrnu"
      },
      "outputs": [],
      "source": [
        "df_knn = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/df_knn.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7MOrHM5SuCW"
      },
      "outputs": [],
      "source": [
        "# split feature and label\n",
        "X = df_knn.drop('CVD0010', axis=1)\n",
        "y = df_knn['CVD0010']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhDgqr1_SueD"
      },
      "outputs": [],
      "source": [
        "# normalize X_train, X_test\n",
        "scaler = StandardScaler()\n",
        "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqSR9oLfSvlJ"
      },
      "outputs": [],
      "source": [
        "# split train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb6flGvESwht"
      },
      "outputs": [],
      "source": [
        "# hyperparameter\n",
        "n_epochs = 1000\n",
        "batch_size = 128\n",
        "latent_dim = X_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUvZEPKmSzg3"
      },
      "outputs": [],
      "source": [
        "# generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.gen = nn.Sequential(\n",
        "            nn.ConvTranspose1d(1, 32, 15, 2, 7),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.ConvTranspose1d(32, 64, 15, 2, 7),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.ConvTranspose1d(64, 128, 15, 1, 7),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.ConvTranspose1d(128, 64, 15, 1, 7),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.ConvTranspose1d(64, 32, 15, 1, 7),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.ConvTranspose1d(32, 1, 15, 1, 7),\n",
        "        )\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                init.kaiming_uniform_(m.weight.data, nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    init.zeros_(m.bias.data)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        noise = noise.unsqueeze(1)\n",
        "        return self.gen(noise).squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agDZpQuQS0iQ"
      },
      "outputs": [],
      "source": [
        "# discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, data_dim=latent_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, 15, 2, 7),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv1d(32, 64, 15, 2, 7),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv1d(64, 32, 15, 1, 7),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv1d(32, 1, 15, 1, 7),\n",
        "\n",
        "            nn.Linear(128, 1),\n",
        "\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, data):\n",
        "        data = data.unsqueeze(1)\n",
        "        return self.disc(data).squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ8KPoQIS15N"
      },
      "outputs": [],
      "source": [
        "# Loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Optimizers\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.00001, betas=(0.5, 0.999))\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.00001, betas=(0.5, 0.999))\n",
        "\n",
        "\n",
        "indices = (y_train == 1)\n",
        "X_real = X_train[indices]\n",
        "y_real = y_train[indices]\n",
        "my_dataset = TensorDataset(torch.Tensor(X_real.values), torch.Tensor(y_real.values))\n",
        "# dataloader\n",
        "dataloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DYfDxonxS3p-"
      },
      "outputs": [],
      "source": [
        "# original gan Train\n",
        "gen_losses = []\n",
        "disc_losses = []\n",
        "for epoch in range(n_epochs):\n",
        "    for i, real_data in enumerate(dataloader):\n",
        "        real_data = real_data[0].float()\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        real_labels = torch.ones(real_data.size(0), 1, requires_grad=False)\n",
        "        fake_labels = torch.zeros(real_data.size(0), 1, requires_grad=False)\n",
        "\n",
        "        # ---------------------\n",
        "        #  First Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        d_optimizer.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = criterion(discriminator(real_data).reshape([X_real.shape[0], 1]), real_labels)\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = torch.randn(real_data.size(0), 128)\n",
        "\n",
        "        # Generate a batch of datas\n",
        "        gen_datas = generator(z)\n",
        "\n",
        "        fake_loss = criterion(discriminator(gen_datas.detach()), fake_labels)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        g_optimizer.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = torch.randn(real_data.size(0), 128)\n",
        "\n",
        "        # Generate a batch of datas\n",
        "        gen_datas = generator(z)\n",
        "\n",
        "        g_loss = criterion(discriminator(gen_datas), real_labels)\n",
        "\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        gen_losses.append(g_loss.item())\n",
        "        disc_losses.append(d_loss.item())\n",
        "\n",
        "        print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gZ1v9xHzS4_Y"
      },
      "outputs": [],
      "source": [
        "# Convert lists to NumPy arrays\n",
        "gen_losses = np.array(gen_losses)\n",
        "disc_losses = np.array(disc_losses)\n",
        "\n",
        "# Plot loss curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(range(len(gen_losses)), gen_losses, label='Generator loss')\n",
        "plt.plot(range(len(disc_losses)), disc_losses, label='Discriminator loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l1fNkL7wm2Q_"
      },
      "outputs": [],
      "source": [
        "# count the frequency of each value in y_train.\n",
        "value_counts = y_train.value_counts()\n",
        "# calculate how much fewer the number of minority classes is than that of majority classes.\n",
        "diff = value_counts[0] - value_counts[1]\n",
        "\n",
        "z = torch.randn(diff, 128)\n",
        "\n",
        "# Generate a batch of datas\n",
        "generated_data_dcgan = generator(z)\n",
        "generated_data_dcgan_df = pd.DataFrame(generated_data_dcgan.detach().numpy(), columns=X_train.columns)\n",
        "\n",
        "# add the generated data to the training set, X_train_dcgan, y_train_gan\n",
        "X_train_dcgan = X_train.append(generated_data_dcgan_df, ignore_index=True)\n",
        "# Supplement y_train with 1.0\n",
        "ones = pd.Series([1.0] * diff)\n",
        "y_train_dcgan = y_train.append(ones, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zihk-_Wgm24b"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "# shuffle\n",
        "X_train_dcgan, y_train_dcgan = shuffle(X_train_dcgan, y_train_dcgan, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rUxsMasgS68N"
      },
      "outputs": [],
      "source": [
        "real_one = X[y == 1]\n",
        "\n",
        "# original data (label==1)\n",
        "tsne_real = real_one # num == 76\n",
        "\n",
        "# smote data (label==1)\n",
        "tsne_real_dcgan = generated_data_dcgan_df # num == 392\n",
        "\n",
        "# TSNE\n",
        "combined_data = pd.concat([tsne_real_dcgan, tsne_real], axis=0)\n",
        "labels = np.array([0] * len(tsne_real_dcgan) + [1] * len(tsne_real))\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_data = tsne.fit_transform(combined_data)\n",
        "\n",
        "# show\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(tsne_data[labels == 0, 0], tsne_data[labels == 0, 1], c='b', label='DCGAN data')\n",
        "plt.scatter(tsne_data[labels == 1, 0], tsne_data[labels == 1, 1], c='r', label='Real data')\n",
        "plt.legend()\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('TSNE comparing real data with DCGAN generated data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uLQM8tQMS75O"
      },
      "outputs": [],
      "source": [
        "# PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "merged_data = np.concatenate((real_one, generated_data_dcgan_df), axis=0)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_data = pca.fit_transform(merged_data)\n",
        "\n",
        "plt.scatter(reduced_data[real_one.shape[0]:, 0], reduced_data[real_one.shape[0]:, 1], label='DCGAN data')\n",
        "plt.scatter(reduced_data[:real_one.shape[0], 0], reduced_data[:real_one.shape[0], 1], label='Real data')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n2QLd33NZheI"
      },
      "outputs": [],
      "source": [
        "# SVM\n",
        "import torch\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score, accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def train_and_evaluate_svm(X_train, y_train, X_test, y_test):\n",
        "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "\n",
        "    # SVM\n",
        "    svm_classifier = SVC(probability=True)\n",
        "\n",
        "    # train\n",
        "    svm_classifier.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "    # test\n",
        "    y_scores = svm_classifier.predict_proba(X_test_tensor.numpy())[:, 1]\n",
        "    y_pred = svm_classifier.predict(X_test_tensor.numpy())\n",
        "\n",
        "    # FPR and TPR for ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test_tensor.numpy(), y_scores)\n",
        "\n",
        "    # AUC\n",
        "    auc = roc_auc_score(y_test_tensor.numpy(), y_scores)\n",
        "\n",
        "    return auc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QpMQc8m3Zous"
      },
      "outputs": [],
      "source": [
        "# original data\n",
        "roc_results = []\n",
        "\n",
        "# Repeat the training and evaluation process 30 times.\n",
        "num_repeats = 30\n",
        "for _ in range(num_repeats):\n",
        "    auc = train_and_evaluate_svm(X_train, y_train, X_test, y_test)\n",
        "    roc_results.append(auc)  # Store the AUC value in the tuple\n",
        "\n",
        "# Calculate the average value of evaluation indicators\n",
        "mean_auc = np.mean([result for result in roc_results])  # Retrieve the AUC value from the tuple\n",
        "\n",
        "print(\"Mean AUC:\", mean_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJz8BSV2Zsn7"
      },
      "outputs": [],
      "source": [
        "# original data\n",
        "roc_results_dcgan = []\n",
        "\n",
        "# Repeat the training and evaluation process 30 times.\n",
        "num_repeats = 30\n",
        "for _ in range(num_repeats):\n",
        "    auc = train_and_evaluate_svm(X_train_dcgan, y_train_dcgan, X_test, y_test)\n",
        "    roc_results_dcgan.append(auc)  # Store the AUC value in the tuple\n",
        "\n",
        "# Calculate the average value of evaluation indicators\n",
        "mean_auc_dcgan = np.mean([result for result in roc_results_dcgan])  # Retrieve the AUC value from the tuple\n",
        "\n",
        "print(\"Mean AUC:\", mean_auc_dcgan)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}